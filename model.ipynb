{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec363921-8144-4113-9ab7-cfb7c3fe2373",
   "metadata": {},
   "source": [
    "#### This module generates tokenizer, defines, trains and saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ef777c-259b-4050-b2d6-bcded73590b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "# Imports\n",
    "import scrape_data_ as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c56ac58-c640-466f-b9d4-14bfc794b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 291.48 MiB, increment: 0.03 MiB\n",
      "Time elapsed: 0.00000 (seconds)\n"
     ]
    }
   ],
   "source": [
    "# Parameters definitions\n",
    "%memit\n",
    "start_time = time.time()\n",
    "\n",
    "MAX_LENGTH = 1000\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "EMBEDDING_DIM = 64\n",
    "VOCAB_SIZE = 50000\n",
    "MAX_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 5\n",
    "\n",
    "FILE_PATH = 'dataset.tsv'\n",
    "TOKENIZER_PATH = 'recipe_tokenizer.json'\n",
    "MODEL_PATH = 'classifier'\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671b27bd-3247-4886-9475-58911c9e227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 291.58 MiB, increment: 0.10 MiB\n",
      "Time elapsed: 0.00000 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "\n",
    "def generate_tokenizer(texts_to_fit, vocab_size = VOCAB_SIZE, tokenizer_path = TOKENIZER_PATH  ):\n",
    "    # Create a Tokenizer object with the specified vocabulary size and out-of-vocabulary token\n",
    "    tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "\n",
    "    # Fit the tokenizer on the input texts\n",
    "    tokenizer.fit_on_texts(texts_to_fit)\n",
    "\n",
    "    # Save the tokenizer configuration to a file\n",
    "    with open(tokenizer_path, 'w+') as tokenizer_file:\n",
    "        tokenizer_file.write(tokenizer.to_json())\n",
    "\n",
    "    # Return the fitted tokenizer\n",
    "    return tokenizer\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3dbd606-802d-430e-970f-d4090581e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 291.67 MiB, increment: 0.00 MiB\n",
      "Time elapsed: 0.00000 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "\n",
    "def get_data(file_path = FILE_PATH):\n",
    "    # Load the dataset into a pandas DataFrame from a TSV file\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "    # Extract the text data and label data from the DataFrame as lists\n",
    "    data, labels =  df[0].astype(str).tolist(), df[1].to_list()\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_size = int(0.8 * len(data))\n",
    "\n",
    "    # Generate a tokenizer based on the training data\n",
    "    tokenizer = generate_tokenizer(data[:train_size], vocab_size = VOCAB_SIZE, tokenizer_path = TOKENIZER_PATH)\n",
    "\n",
    "    # Preprocess the text data using the TextPreprocessor object\n",
    "    processed_data = sc.preprocess_text(data)\n",
    "\n",
    "    # Convert the label data from string to integer arrays using ast.literal_eval()\n",
    "    arr = np.array(labels)\n",
    "    processed_labels = np.array([ast.literal_eval(x) for x in arr])\n",
    "\n",
    "    # Split the preprocessed data and labels into train and test sets\n",
    "    X_train, y_train = processed_data[:train_size], processed_labels[:train_size]\n",
    "    X_test, y_test = processed_data[train_size:], processed_labels[train_size:]\n",
    "\n",
    "    # Return the preprocessed train and test sets\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1022e6-4cf3-444a-a4ee-abf83f946433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 291.68 MiB, increment: 0.00 MiB\n",
      "Time elapsed: 0.00000 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "\n",
    "def create_model():\n",
    "    # Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an embedding layer with the specified input dimension, output dimension, and input length\n",
    "    model.add(layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                               output_dim = EMBEDDING_DIM, \n",
    "                               input_length = MAX_LENGTH))\n",
    "\n",
    "    # Add a flatten layer to flatten the output of the embedding layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Add a dense layer with 32 units and SELU activation function\n",
    "    model.add(layers.Dense(32, activation='selu'))\n",
    "\n",
    "    # Add another dense layer with 64 units and SELU activation function\n",
    "    model.add(layers.Dense(64, activation='selu'))\n",
    "\n",
    "    # Add a dense layer with 3 units and softmax activation function\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model with Adam optimizer, binary crossentropy loss, and accuracy metric\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "#     # Define the optimizer\n",
    "#     sgd = SGD()  \n",
    "\n",
    "#     # Compile the model with the chosen loss function and optimizer\n",
    "#     model.compile(loss='hinge', optimizer=sgd, metrics=['accuracy']) \n",
    "    \n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Return the compiled model\n",
    "    return model\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53db6a31-c2d7-42d5-9b52-8e755cc5d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 291.69 MiB, increment: 0.00 MiB\n",
      "Time elapsed: 0.00000 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    model = create_model()\n",
    "    \n",
    "    # create the EarlyStopping callback\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience = PATIENCE, mode='min')\n",
    "\n",
    "    # fit the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
    "\n",
    "    # Return the trained model and history\n",
    "    return model, history\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7101761-e2f8-421e-acc2-59e95eda88ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 291.69 MiB, increment: 0.00 MiB\n",
      "Time elapsed: 0.00000 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "\n",
    "# Save the model\n",
    "def save_model(model):\n",
    "    model.save(model_path)\n",
    "    \n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83e698b-f195-4af5-a85e-1311ce49eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 291.69 MiB, increment: 0.00 MiB\n",
      "Time elapsed: 18.35678 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "# Get the train and test data\n",
    "X_train, y_train, X_test, y_test = get_data()\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ccb1f80-a92a-45d9-8903-81feba0d5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 484.36 MiB, increment: 0.00 MiB\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 64)          3200000   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2048032   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,250,339\n",
      "Trainable params: 5,250,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "594/594 [==============================] - 60s 98ms/step - loss: 0.0543 - accuracy: 0.9693 - val_loss: 0.0093 - val_accuracy: 0.9962\n",
      "Epoch 2/50\n",
      "594/594 [==============================] - 50s 84ms/step - loss: 0.0047 - accuracy: 0.9976 - val_loss: 0.0076 - val_accuracy: 0.9962\n",
      "Epoch 3/50\n",
      "594/594 [==============================] - 47s 80ms/step - loss: 0.0021 - accuracy: 0.9988 - val_loss: 0.0089 - val_accuracy: 0.9954\n",
      "Epoch 4/50\n",
      "594/594 [==============================] - 45s 76ms/step - loss: 0.0014 - accuracy: 0.9992 - val_loss: 0.0096 - val_accuracy: 0.9973\n",
      "Epoch 5/50\n",
      "594/594 [==============================] - 45s 76ms/step - loss: 0.0012 - accuracy: 0.9992 - val_loss: 0.0117 - val_accuracy: 0.9940\n",
      "Epoch 6/50\n",
      "594/594 [==============================] - 46s 77ms/step - loss: 0.0011 - accuracy: 0.9991 - val_loss: 0.0136 - val_accuracy: 0.9937\n",
      "Epoch 7/50\n",
      "594/594 [==============================] - 46s 78ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.0158 - val_accuracy: 0.9935\n",
      "Time elapsed: 339.81760 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "# Train the model\n",
    "trained_model, training_history = train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5a4dd2-3868-4677-9727-9bf9df6d8a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 577.30 MiB, increment: 0.00 MiB\n",
      "INFO:tensorflow:Assets written to: classifier\\assets\n",
      "Time elapsed: 1.61927 (seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%memit\n",
    "start_time = time.time()\n",
    "# Save the model\n",
    "trained_model.save(MODEL_PATH)\n",
    "\n",
    "# Print the time of execution\n",
    "end_time =   time.time() \n",
    "print(f'Time elapsed: {end_time - start_time :.5f} (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c1833-b2c1-42d2-8d08-affc2f7e251a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef470c3-7c5a-46e7-a3d7-84a493628b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
